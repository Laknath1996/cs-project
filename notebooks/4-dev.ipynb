{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora.models.roberta_scratch import get_model\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import scipy as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config, rank=1):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # self.B = torch.randn((config.hidden_size+1, config.hidden_size+1))\n",
    "\n",
    "        self.U = torch.nn.Parameter(\n",
    "            data=torch.randn((config.hidden_size+1, rank)),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.V = torch.nn.Parameter(\n",
    "            data=torch.randn((rank, config.hidden_size+1)),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        value_layer = self.value(hidden_states)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        X = torch.cat((hidden_states, torch.ones_like(hidden_states)[:, :, :1]), axis=-1)\n",
    "        M = torch.matmul(self.U, self.V)\n",
    "        attention_scores = torch.matmul(\n",
    "                torch.matmul(X, M),\n",
    "                X.transpose(-1, -2)\n",
    "            )\n",
    "\n",
    "        # scale the attention scores\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask.view(attention_scores.shape[0], 1, attention_scores.shape[-1])\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def get_approx_model(model, rank=1):\n",
    "    approx_model = deepcopy(model)\n",
    "    if rank == 769:\n",
    "        return approx_model\n",
    "    else:\n",
    "        for i in range(model.roberta_encoder.config.num_hidden_layers):\n",
    "            old = approx_model.roberta_encoder.encoder.layer[i].attention.self\n",
    "\n",
    "            # get the original B matrix\n",
    "            Wq = torch.vstack([old.query.weight.transpose(0, 1), old.query.bias]).cpu()\n",
    "            Wk = torch.vstack([old.key.weight.transpose(0, 1), old.key.bias]).cpu()\n",
    "            B = (Wq @ Wk.T).cpu().detach().numpy()\n",
    "\n",
    "            # low-rank approx\n",
    "            U, s, Vs = scp.sparse.linalg.svds(B, k=rank, which='LM')\n",
    "            U = U @ np.diag(s)\n",
    "            V = Vs\n",
    "\n",
    "            approx_model.roberta_encoder.encoder.layer[i].attention.self = SelfAttention(model.roberta_encoder.config)\n",
    "            approx_model.roberta_encoder.encoder.layer[i].attention.self.U.data = torch.from_numpy(U)\n",
    "            approx_model.roberta_encoder.encoder.layer[i].attention.self.V.data = torch.from_numpy(V)\n",
    "            approx_model.roberta_encoder.encoder.layer[i].attention.self.value.weight = old.value.weight\n",
    "            approx_model.roberta_encoder.encoder.layer[i].attention.self.value.bias = old.value.bias\n",
    "        return approx_model\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    num_correct_preds = 0\n",
    "    num_test_samples = 0\n",
    "    progress = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in progress:\n",
    "            ids = data[\"input_ids\"].long().to(device)\n",
    "            token_type_ids = data[\"token_type_ids\"].long().to(device)\n",
    "            masks = data[\"attention_mask\"].long().to(device)\n",
    "            targets = data[\"label\"].long().to(device)\n",
    "            batchsize = len(targets)\n",
    "\n",
    "            outputs = model(ids, masks, token_type_ids)\n",
    "\n",
    "            num_test_samples += batchsize\n",
    "            num_correct_preds += (\n",
    "                (torch.argmax(outputs, axis=1) == targets)\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "                .sum()\n",
    "            )\n",
    "\n",
    "            progress.set_description(f\"[{i + 1}/{len(dataloader)}] \")\n",
    "    return num_correct_preds / num_test_samples\n",
    "\n",
    "# load original model\n",
    "args = OmegaConf.load(\"/cis/home/adesilva/ashwin/research/cs-project/config.yaml\")\n",
    "weights_path = \"/cis/home/adesilva/ashwin/research/cs-project/outputs/2024-04-25/08-40-45/scratch.pth\"\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model(args)\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# prepare the validation dataset\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "dataset = load_dataset(\"nyu-mll/glue\", \"sst2\")\n",
    "dataset = dataset.map(\n",
    "    lambda e: tokenizer(\n",
    "        e[\"sentence\"],\n",
    "        None,\n",
    "        max_length=256,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True,\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"],\n",
    "        )\n",
    "dataloader = DataLoader(\n",
    "            dataset[\"validation\"],\n",
    "            batch_size=16,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    count = []\n",
    "    for p in model.parameters():\n",
    "        count.append(p.numel())\n",
    "    return sum(count[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_model = get_approx_model(model, rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8373487239648264"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(approx_model)/count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43515650"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36437774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = []\n",
    "for p in approx_model.parameters():\n",
    "    count.append(p.numel())\n",
    "sum(count[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoBERTaClassifier(\n",
       "  (roberta_encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): Tanh()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 1,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta_encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list = [1, 5, 10, 100, 769]\n",
    "param_list = []\n",
    "acc_list = []\n",
    "for i, rank in enumerate(rank_list):\n",
    "    approx_model = get_approx_model(model, rank)\n",
    "    param_list.append(count_parameters(approx_model))\n",
    "    approx_model.to(device)\n",
    "    acc = evaluate(approx_model, dataloader)\n",
    "    acc_list.append(acc)\n",
    "\n",
    "param_list = np.array(param_list)\n",
    "param_list = param_list / param_list.max()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(param_list, acc_list, marker='o', ms=5)\n",
    "ax.set_xlabel(\"rank\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim([0.5, 0.95])\n",
    "plt.show()\n",
    "plt.savefig(\"perf-curve.png\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
